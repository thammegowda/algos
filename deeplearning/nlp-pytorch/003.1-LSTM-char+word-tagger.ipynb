{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 003.1 - LSTM Tagger with Word+Char Embeddings\n",
    "\n",
    "Thamme Gowda , April 26, 2018\n",
    "\n",
    "Taken from Pytorch tutorials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Version 0.4.0\n",
      "Cuda is available: True\n",
      "CUDA_VISIBLE_DEVICES : 0,2\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "print(f'Torch Version {torch.__version__}')\n",
    "assert int(torch.__version__.split('.')[1]) >= 4 # should be greater than equal to 0.4\n",
    "print(f'Cuda is available: {torch.cuda.is_available()}')\n",
    "print(f'CUDA_VISIBLE_DEVICES : {os.environ[\"CUDA_VISIBLE_DEVICES\"]}')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': 0, 'dog': 1, 'ate': 2, 'the': 3, 'apple': 4, 'Everybody': 5, 'read': 6, 'that': 7, 'book': 8}\n",
      "16 {'E': 0, 'T': 1, 'a': 2, 'b': 3, 'd': 4, 'e': 5, 'g': 6, 'h': 7, 'k': 8, 'l': 9, 'o': 10, 'p': 11, 'r': 12, 't': 13, 'v': 14, 'y': 15}\n"
     ]
    }
   ],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    return torch.tensor([to_ix[w] for w in seq], dtype=torch.long, device=device)\n",
    "\n",
    "training_data = [\n",
    "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
    "]\n",
    "word_to_ix = {}\n",
    "for sent, tags in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "\n",
    "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}\n",
    "\n",
    "\n",
    "uniq_chars = set(ch for seq, _ in training_data for word in seq for ch in word) # read left to right\n",
    "char_to_ix = {ch: ix for ix, ch in enumerate(sorted(uniq_chars))}\n",
    "\n",
    "print(word_to_ix)\n",
    "print(len(char_to_ix), char_to_ix)\n",
    "\n",
    "\n",
    "# These will usually be more like 32 or 64 dimensional.\n",
    "# We will keep them small, so we can see how the weights change as we train.\n",
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharWordLSTMTagger(nn.Module):\n",
    "    '''\n",
    "    Character + Word Embedding based LSTM Tagger\n",
    "    '''\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, word2ix, char2ix, tag2ix):\n",
    "        super(CharWordLSTMTagger, self).__init__()\n",
    "        self.num_dirs = 1\n",
    "        self.num_layers = 2\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.char2ix = char2ix\n",
    "        self.word2ix = word2ix\n",
    "        self.tag2ix = tag2ix\n",
    "        self.ix2tag = {v: k for k,v in tag2ix.items()}\n",
    "        vocab_size, alphabet_size, tagset_size = len(word2ix), len(char2ix), len(tag2ix)\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.char_embeddings = nn.Embedding(alphabet_size, embedding_dim)\n",
    "        self.char_lstm = nn.LSTM(embedding_dim, hidden_dim,\n",
    "                                 num_layers=self.num_layers,\n",
    "                                 bidirectional=self.num_dirs == 2)\n",
    "        self.char_hidden = self.init_hidden() # Hidden state for char LSTM\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim. \n",
    "        # we concatenate character and word embeddings\n",
    "        self.lstm = nn.LSTM(self.num_dirs * embedding_dim + hidden_dim, hidden_dim,\n",
    "                            num_layers=self.num_layers,\n",
    "                            bidirectional=self.num_dirs == 2)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(self.num_dirs * hidden_dim, tagset_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers*directions, minibatch_size, hidden_dim)\n",
    "        return (torch.zeros(self.num_layers * self.num_dirs, 1, self.hidden_dim, device=device),\n",
    "                torch.zeros(self.num_layers * self.num_dirs, 1, self.hidden_dim, device=device))\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        word_reprs = []\n",
    "        for word in sentence:\n",
    "            word_idx = torch.tensor([self.word2ix[word]], dtype=torch.long, device=device)\n",
    "            word_embed = self.word_embeddings(word_idx)\n",
    "            self.char_hidden = self.init_hidden()\n",
    "            ch_ixs = [self.char2ix[ch] for ch in word]\n",
    "            ch_seq = torch.tensor(ch_ixs, dtype=torch.long, device=device)\n",
    "            ch_embeds = self.char_embeddings(ch_seq)\n",
    "            ch_repr, self.char_hidden = self.char_lstm(ch_embeds.view(len(word), 1, -1), self.char_hidden)\n",
    "            word_repr = torch.cat((word_embed, ch_repr[-1]), dim=1) # char LSTM output's last time stamp output\n",
    "            word_reprs.append(word_repr)\n",
    "        embeds = torch.cat(word_reprs)\n",
    "        #print(len(sentence), embeds.size())\n",
    "        lstm_out, self.hidden = self.lstm(embeds.view(len(sentence), 1, -1), self.hidden)\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=0)\n",
    "        return tag_scores\n",
    "    \n",
    "    def tag(self, sentence):\n",
    "        vals, idx = self(sentence).max(dim=1)\n",
    "        return [self.ix2tag[i.item()] for i in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5695, -1.6457, -1.6651],\n",
      "        [-1.6009, -1.6211, -1.6356],\n",
      "        [-1.6193, -1.5984, -1.6045],\n",
      "        [-1.6261, -1.5889, -1.5791],\n",
      "        [-1.6327, -1.5942, -1.5662]], device='cuda:0')\n",
      "['The', 'dog', 'ate', 'the', 'apple']\n",
      "['DET', 'NN', 'V', 'DET', 'NN']\n",
      "tensor([[-1.1046, -5.7964, -5.0518],\n",
      "        [-7.5461, -0.6958, -5.3074],\n",
      "        [-4.9072, -5.7974, -0.0184],\n",
      "        [-0.4229, -6.2712, -5.0101],\n",
      "        [-5.1899, -0.7065, -8.2660]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "tagger = CharWordLSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, word_to_ix, char_to_ix, tag_to_ix)\n",
    "tagger.to(device)\n",
    "loss_func = nn.NLLLoss()\n",
    "optimizer = optim.SGD(params=tagger.parameters(), lr=0.5)\n",
    "\n",
    "print(tagger(training_data[0][0]))\n",
    "for e in range(200):\n",
    "    for seq, tags in training_data:\n",
    "        y_gold = prepare_sequence(tags, tag_to_ix)\n",
    "        tagger.zero_grad()\n",
    "        tagger.hidden = tagger.init_hidden()\n",
    "        \n",
    "        y_pred = tagger(seq)\n",
    "        loss = loss_func(y_pred, y_gold)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "xseq = training_data[0][0]\n",
    "print(xseq)\n",
    "print(tagger.tag(xseq))\n",
    "print(tagger(xseq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "1. Learn how to use CRF + viterbi decoding with these features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
