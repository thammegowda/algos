%%%%%%%%%%%
%% Home work template for Graduate School
%% Author : Thamme Gowda N.
%% Originally from  https://github.com/thammegowda/hw-tex-templ
%%%%%%%%%%%%%%

\documentclass[letterpaper,doc,notimes]{apa6}
%Required by APA6 package
\usepackage[normalem]{ulem}
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{adjustbox}

%Oft-used, oft-abused
\usepackage{afterpage}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{censor}
\usepackage{color}
\usepackage{csquotes}
\usepackage{enumitem}
\usepackage{float}
\usepackage{hyperref}
\usepackage{lmodern}
%\usepackage{media9}
\usepackage{multirow}
\usepackage{outlines}
\usepackage{pdfpages}
\usepackage{placeins}
\usepackage{soul}
\usepackage{tabularx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{xcolor}


\setenumerate[1]{label=\Roman*.}
\setenumerate[2]{label=\Alph*.}
\setenumerate[3]{label=\roman*.}
\setenumerate[4]{label=\alph*.}

\title{ \textbf{ USC CSCI 567 HOMEWORK 1 SOLUTIONS} }
\shorttitle{USC CSCI567 FALL16 HW1}
\author{\textsc{ThammeGowda Narayanaswamy}}
\affiliation{ tnarayan@usc.edu \\ ID : 2074-6694-39 \\ Department of Computer Science \\ Viterbi School of Engineering \\ University of Southern California \\ Los Angeles, CA }

%\note{September 13, 2016}
\note{\today}
\authornote{Produced for Fall 2016 section of CSCI 567, ``Machine Learning'', taught by Dr. Yan Liu at the University of Southern California}


\begin{document}

\maketitle
\newpage

\section{1. Density Estimation }
\subsection{a. } 
\begin{description}
	\item[$\bullet \beta$ Distribution:]
	Given that the i.i.d samples follow $\beta$ distribution.
	The general form of PDF of $\beta$ distribution is
	 $B(\alpha, \beta) = C \cdot x^{\alpha-1} (1-x)^{\beta-1} $, Where $C$ is a constant
	 \newline Given that, $\beta=1$ and we $\alpha$ is unkown (thus we need to estimate using MLE).
	 $\implies B(\alpha, 1) = C \cdot x^{\alpha-1} (1-x)^{0} = C x^{\alpha-1}$
	\newline We know that, the likelihood for parameter $\theta$ is maximum when the first order derivative of the distribution function is $0$
	\begin{align*}
		\dfrac{d B}{d\alpha} &= 0 \\
		C \dfrac{d}{d\alpha}[x^{\alpha-1}] &= 0 \\
		C \cdot (\alpha-1) \cdot x^{\alpha-2} \cdot 1 &= 0	
	\end{align*}
	We know C $\ne$ 0, and $x^{\alpha-2} = 0$ is undefined.
	$\newline \implies \alpha = 1$
	\item[$\bullet$ Normal Distribution:]
	Given that i.i.d samples follow normal distribution with parameters $N(\theta, \theta)$. 
	\newline The general form of PDF of the normal distribution is 
		$ N(\mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}} $
	\newline Thus, $ N(\theta, \theta) = \frac{1}{\sqrt{2\pi\theta}} e^{-\frac{(x-\theta)^2}{2\theta}}$ 
	\newline We know that, the likelihood for parameter $\theta$ is maximum when the first order derivative of the distribution function is $0$.
	\newline Thus, the $\theta_{MLE}$ can be estimated using
$\dfrac{d N(\theta,\theta)}{d\theta} = 0$

For the sake of simplifying the calculations, using logarithm on function $N(\theta, \theta)$
\begin{align*}
	\dfrac{d \log{N(\theta,\theta)}}{d\theta} &= 0 \\
	\dfrac{d}{d\theta}[\log{\{\frac{1}{\sqrt{2\pi\theta}} e^{-\frac{(x-\theta)^2}{2\theta}}\}} ] &= 0 \\
	\dfrac{d}{d\theta}[-\log\sqrt{2\pi\theta} -\frac{(x-\theta)^2}{2\theta}] &= 0 \\
	-\dfrac{d}{d\theta}[\log\sqrt{2\pi\theta}] - \frac{1}{2} \dfrac{d}{d\theta}[\frac{1}{\theta} (x-\theta)^2] &= 0 \\
	- \frac{1}{\sqrt{2\pi\theta}} \dfrac{d}{d\theta}[(2\pi\theta)^\frac{1}{2}] - \frac{1}{2}( \frac{-1}{\theta^2} (x-\theta)^2 + \frac{1}{\theta} \cdot 2 \cdot (x-\theta) \cdot -1 ) &= 0 \\
	- \frac{1}{\sqrt{2\pi\theta}} \cdot \frac{1}{2} \cdot \frac{1}{\sqrt{2\pi\theta}} \cdot 2\pi + \frac{(x-\theta)^2}{2\theta^2}  + \frac{(x-\theta)}{\theta} &= 0 \\
	-\frac{1}{2 \theta} + \frac{(x-\theta)^2}{2\theta^2}  + \frac{(x-\theta)}{\theta} &= 0 \\
	\frac{-\theta + (x-\theta)^2 + (x-\theta)\theta}{2\theta^2} &= 0 \\
	-\theta + x^2 - 2x\theta + \theta^2 +x\theta -\theta^2 &= 0 \\
	x^2  -\theta - x\theta &= 0 \\
	\theta + x\theta &= x^2 \\
	\implies \theta &= \frac{x^2}{1+x}
\end{align*}


	
\end{description}

\subsection{b.}

\section{2. Naive Bayes}
\subsection{(a)}

\subsection{(b)}

\section{3. Nearest Neighbor}

\subsection{(a)}

Data Preprocessing:
	The input data are normalized and standardized. The mean of the coordinates, denoted by $(\mu_x, \mu_y)$ and the standard deviations of co-ordinates, denoted by $(\sigma_x, \sigma_y)$, are computed for the given data points as follows 
	\begin{align*}
		\mu_x & = \frac{1}{n}\sum_{1}^{n} x_i   &  \sigma_x &= \sqrt{ \frac{1}{n-1} \sum_{1}^{n} (x_i - \mu_x)^2 } \\
		\mu_y & = \frac{1}{n}\sum_{1}^{n} y_i   &  \sigma_y &= \sqrt{\frac{1}{n-1} \sum_{1}^{n} (y_i - \mu_y)^2 }\\
	\end{align*}

For the given data points, values of these variables are:
	\begin{align*}
		n &= 13  \\
		(\mu_x, \mu_y )& = (12.7692, 12.3077)  \\
		(\sigma_x, \sigma_y) &= (20.7169, 25.9306)
	\end{align*}

The position of each students, $(x_i, y_i)$ is transformed to a new normalized co-ordinate system by shifting the origin to $(\mu_x, \mu_y)$ and scaling the magnitudes by $(\frac{1}{\sigma_x},   \frac{1}{\sigma_y})$ as follows:
\begin{equation*}
	(x_{i'}, y_{i'}) =  (\frac{x_i - \mu_x}{\sigma_x}, \frac{y_i - \mu_y}{\sigma_y})
\end{equation*}


For any two points, $(x_1, y_1)$ and $(x_2, y_2)$,


The $L_1$ distance between these two points is given by:
$L_1 = |x_1 - x_2| + |y_1 - y_2|)$ 

The $L_2$ distance between these two points is:
$L_2 = \sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}$

Given that the new students location is $ (20, 7)$. 
In the normalized-standardized coordinate system, it is transformed to position (0.3490, -0.2047).

Using the above formula, the distances are computed and the data are tabulated below:

\begin{tabular}{| c | c | c| c || r || r |}
	\hline			
	USCID  & $(x_i, y_i)$ & Major & $(x_{i'}, y_{i'})$ & $L_1$ $(0.3490, -0.2047)$ & $L_2$  $(0.3490, -0.2047)$\\
	\hline
	S1  &  (0, 49)  &  Mat & (-0.6164, 1.4150) & 0.965 + 1.620 = 2.585 & $\sqrt{0.932 + 2.623}$ = 1.886 \\
	S2  &  (-7, 32) &  Mat & (-0.9543, 0.7594) & 1.303 + 0.964 = 2.267 & $\sqrt{1.698 + 0.930}$ = 1.621 \\
	S3  &  (-9, 47) &  Mat & (-1.0508, 1.3379) & 1.400 + 1.543 = 2.942 & $\sqrt{1.959 + 2.380}$ = 2.083 \\
	S4  &  (29, 12) &  Ele & (0.7835, -0.0119)& 0.434 + 0.193 = 0.627 & $\sqrt{0.189 + 0.037}$ = 0.475  \\
	S5  &  (49, 31) &  Ele & (1.7488, 0.7209) & 1.400 + 0.926 = 2.325 & $\sqrt{1.960 + 0.857}$ = 1.678 \\
	S6  &  (37, 38) &  Ele & (1.1696, 0.9908) & 0.821 + 1.196 = 2.016 & $\sqrt{0.673 + 1.429}$ = 1.450 \\
	S7  &  (8, 9)   &  CSc & (-0.2302, -0.1276) & 0.579 + 0.077 = 0.656 & $\sqrt{0.335 + 0.006}$ = 0.584 \\
	S8  &  (13, -1) &  CSc & (0.0111, -0.5132) &  0.338 + 0.309 = 0.646 &$\sqrt{0.114 + 0.095}$ = 0.458\\
	S9  &  (-6, -3) &  CSc & (-0.9060, -0.5903) & 1.255 + 0.386 = 1.641 &$\sqrt{1.575 + 0.149}$ = 1.313 \\
	S10  & (-21, 12) & CSc & (-1.6300, -0.0119) & 1.979 + 0.193 = 2.172 & $\sqrt{3.917 + 0.037}$ = 1.988\\
	S11  & (27, -32) & Eco & (0.6869, -1.7087) & 0.338 + 1.504 = 1.842 & $\sqrt{0.114 + 2.262}$ = 1.541\\
	S12  &  (19,-14) &  Eco & (0.3008, -1.0145) & 0.048 + 0.810 = 0.858 & $\sqrt{0.002 + 0.656}$ = 0.811\\
	S13  &  (27,-20) &  Eco & (0.6869, -1.2459) & 0.338 + 1.041 = 1.379 & $\sqrt{0.114 + 1.084}$ = 1.095\\
	\hline
\end{tabular}


Our predictions of Major of student at $(20, 7)$ for $K= 1, 5$ are computed and tabulated below


\begin{tabular}{| c || c | c |}
	\hline
	Metric 			& K nearest students & Predicted major Class \\
	\hline
	K=1 using $L_1$ & S4 &  Electrical Engineering \\
	K=1 using $L_2$ & S8 & Computer Science \\
	K=5 using $L_1$ & S4, S7, S8, S12 and S13 & Computer Science \\
	K=5 using $L_2$ &  S8, S4, S7, S12 and S13 & Computer Science\\
	\hline
\end{tabular}

\subsection{(b)}
Given/Known facts from problem statement:
\\ There are D dimensions in the vector space
\\ $x$ is a data point, and there is a D-sphere centered at $x$
\\  $V$ is the volume of the sphere.
\\ $N$ is the total number of points in the vector space
\\ $N_c$ is the total number of points in the vector space which have label $c$
\\ $K$ is the total number of points inside the D-sphere
\\ $K_c$ is the total number of points inside the D-sphere which have label $c$
\\ $p(x | Y = c) = \frac{K_c}{N_c V}$
\\ $p(Y = c) = \frac{N_c}{N}$


\begin{description}
	\item[$\bullet$ Unconditional probability density $p(x)$] 
	Using the marginal probability distribution, the $p(x)$ can be rewritten as summation of all marginals.
	\begin{align*}
		p(x) & = \sum_{c=1}^{c=C} P(x, Y=c) \\
			 & = \sum_{c=1}^{c=C} P(x | Y=c) \times P(Y=c) \\
			 & = \sum_{c=1}^{c=C} \frac{K_c}{N_c V} \times \frac{N_c}{N} \\
			 & = \frac{1}{N V}\sum_{c=1}^{c=C} K_c \\
		p(x) & = \frac{1}{N} \times \frac{K}{V}
	\end{align*}

	\item[$\bullet$ Posterior probability of class membership]
	
	Using Bayes' rule, 
	\begin{align*}
		p(Y=c|x) & = \frac{p(x | Y=c) p(Y=c) }{p(x)}\\
	 		& = p(x | Y=c) \times p(Y=c) \times \frac{1}{p(x)} \\
			& = \frac{K_c}{N_c V} \times \frac{N_c}{N} \times \frac{NV}{K} \\
			& = \frac{K_c}{K}
	\end{align*}
\end{description}


\section{4. Decision Tree}
\subsection{(a)}

The entropy at the root node of decision tree is,\\
	\begin{align*}
	 E(root) = & E (H=2, L=2) \\	
	         = & -\frac{2}{4} \log_2\frac{2}{4} -\frac{2}{4} \log_2\frac{2}{4} \\
	         = & 1
	\end{align*}

Information Gain with attribute $Weather$ and $Traffic$ is
	\begin{align*}
		 Gain(Weather) = & E(root) - \frac{2}{4} E_{W=Sunny}(H=1, L=1)  \\ 
				 & - \frac{2}{4} E_{W=Rainy}(H=1, L=1) \\
				 = & 1 - \frac{1}{2} (-1) - \frac{1}{2} (-1) \\
		 Gain(Weather)	= & 0 \\
		 Gain(Traffic) = & E(root) - \frac{2}{4} E_{T=Heavy}(H=2, L=0) \\ & - \frac{2}{4} E_{T=Light}(H=0, L=2) \\
 		  = & 1 - 0 - 0 \\
		 Gain(Traffic) = & 1
	\end{align*}

Since Information Gain of $Traffic$ is higher than $Weather$, I will split based on $Traffic$ at first.

\subsection{(b)}
\subsection{(c)}

\section{4. Programming}
\subsection{(a) Data Inspection}
\begin{description}
	\item[1] How many attributes are there
	\newline There are 10 attributes in each record (including the id).
	
	\item[2] Do you think that all attributes are meaningful for the classification? If not, explain why.
	The \textit{id} attribute is definitely not useful for classification. The reason is obvious since \textit{id} of a record is meant for identification and it does not explain the nature of glass (in other words, \textit{id} is not a function of its behavior).
	 Since I do not have expertise in the domain of glass manufacturing, I would not try to decide the usefulness or uselessness of other attributes in the dataset at this point of time, instead I would let my statistical learning model to learn it. Hence I will supply all the attributes to my learning machine.
	 
	 \item[3] How many classes are? Class is a type of glass.
		 There are \textbf{6} classes of glasses in the dataset provided on blackboard. Originally, in UCI repository there were \textbf{7} classes, but the class with label \textbf{4} is missing in the dataset provided in blackboard.
	
	\item[4] Please explain the class distribution. Which class is majority? Do you think that it can be considered as a uniform distribution?
	The distribution of classes are as follows:
	\textbf{Training Data}:\newline
	The training data has 196 examples. The distribution is: \newline
	\begin{tabular}{| c | c | c| }
		\hline
		Class & Count & Probability \\
		\hline \hline
		1     &  67   &  0.3418 \\ \hline
		2     &  72   &  0.3673 \\ \hline
		3     &  14   &  0.0714 \\ \hline
		5     &  10   &  0.0510 \\ \hline
		6     &   6   &  0.0306 \\ \hline
		7     &  26   &  0.1326 \\ \hline
	\end{tabular} \newline
	As we can infer from the probability column, the classes in training dataset are not distributed uniformly.

	\textbf{Testing Data}: \newline
	The testing data has 18 examples. The distribution is: \newline
	\begin{tabular}{| c | c | c| }
		\hline
		Class & Count & Probability \\
		\hline \hline
		1     &  3   &  $\frac{1}{6}$ \\ \hline
		2     &  3   &  $\frac{1}{6}$ \\ \hline
		3     &  3   &  $\frac{1}{6}$ \\ \hline
		5     &  3   &  $\frac{1}{6}$ \\ \hline
		6     &  3   &  $\frac{1}{6}$ \\ \hline
		7     &  3   &  $\frac{1}{6}$ \\ \hline
	\end{tabular} \newline
	As we can infer from the probability column, the classes in testing dataset are distributed uniformly with probability of $\frac{1}{6}$.
		
	
\end{description}
\subsection{(b) Performance Comparison}



\end{document}