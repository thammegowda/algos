%%%%%%%%%%%
%% Home work template for Graduate School
%% Author : Thamme Gowda N.
%% Originally from  https://github.com/thammegowda/hw-tex-templ
%%%%%%%%%%%%%%

\documentclass[letterpaper,doc,notimes]{apa6}
%%\documentclass[tikz]{standalone}
\usepackage{tikz}
%Required by APA6 package
\usepackage[normalem]{ulem}
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{adjustbox}

%Oft-used, oft-abused
\usepackage{afterpage}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{censor}
\usepackage{color}
\usepackage{csquotes}
\usepackage{enumitem}
\usepackage{float}
\usepackage{hyperref}
\usepackage{lmodern}
%\usepackage{media9}
\usepackage{multirow}
\usepackage{outlines}
\usepackage{pdfpages}
\usepackage{placeins}
\usepackage{soul}
\usepackage{tabularx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{xcolor}
\usepackage{mathtools}


\setenumerate[1]{label=\Roman*.}
\setenumerate[2]{label=\Alph*.}
\setenumerate[3]{label=\roman*.}
\setenumerate[4]{label=\alph*.}

\title{ \textbf{ USC CSCI 567 HOMEWORK 1 SOLUTIONS} }
\shorttitle{USC CSCI567 FALL16 HW1}
\author{\textsc{ThammeGowda Narayanaswamy}}
\affiliation{ tnarayan@usc.edu \\ ID : 2074-6694-39 \\ Department of Computer Science \\ Viterbi School of Engineering \\ University of Southern California \\ Los Angeles, CA }

%\note{September 13, 2016}
\note{\today}
\authornote{Produced for Fall 2016 session of CSCI 567, ``Machine Learning'', taught by Dr. Yan Liu at the University of Southern California}


\begin{document}

\maketitle
\newpage

\section{1. Density Estimation }
\subsection{a. } 
\begin{description}
	\item[$\bullet \beta$ Distribution:]
	Given that the i.i.d samples follow $\beta$ distribution.
	The general form of PDF of $\beta$ distribution is
\begin{gather*}
 B(\alpha, \beta) = \frac{x^{\alpha-1} (1-x)^{\beta-1} }{ \frac{\Gamma(\alpha) \Gamma(\beta)}{\Gamma(\alpha +\beta)}} \\
		 \text{Given that, $\beta=1$ and $\alpha$ is unkown (thus we need to estimate using MLE).} \\
	   \implies B(\alpha, 1) = \frac{x^{\alpha-1}}{\frac{\Gamma(\alpha) \Gamma(1)}{\Gamma(\alpha +1)}} \\
				= \frac{x^{\alpha-1}}{\frac{1}{\alpha}} \\	 
				= \alpha x^{\alpha-1} \\	 
	   \text{Likelihood of this distribution is } 
	   L(\alpha) = \prod_{1}^{n} \alpha x_i^{\alpha-1}\\
	   \text{log-likelihood of this distribution is } 
		l(\alpha) = \log \prod_{1}^{n} \alpha x_i^{\alpha-1}\\ 
		        = \sum_{1}^{n} [\log \alpha + (\alpha-1) \log x_i] \\
		        = n \log \alpha + (\alpha-1) \sum_{1}^{n} \log x_i \\
		 \text{Max log-likelihood of this distribution is when first order derivative is 0 } \\
		 \dfrac{d}{d\alpha} [ n \log \alpha + (\alpha-1) \sum_{1}^{n} \log x_i ] = 0 \\
		\frac{n}{\alpha} + \sum_{1}^{n} \log x_i = 0 \\
		\implies \hat{\alpha} = \frac{-n}{\sum_{1}^{n} \log x_i}
	 \end{gather*}

	\item[$\bullet$ Normal Distribution:]
	Given that i.i.d samples follow normal distribution with parameters $N(\theta, \theta)$. 
	\newline The general form of PDF of the normal distribution is 
		$ N(\mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}} $
	\newline Thus, $ N(\theta, \theta) = \frac{1}{\sqrt{2\pi\theta}} e^{-\frac{(x-\theta)^2}{2\theta}}$ 
	\newline The likelihood of this distribution is 
	$L(\theta) = \prod_{i}^{n} \frac{1}{\sqrt{2\pi\theta}} e^{-\frac{(x_i-\theta)^2}{2\theta}} $
	\newline the log-likelihood is 
\begin{align*}
l(\theta) = & \log [ \prod_{i}^{n} \frac{1}{\sqrt{2\pi\theta}} e^{-\frac{(x_i-\theta)^2}{2\theta}}] \\
		   = & \sum_{i}^{n} \log [\frac{1}{\sqrt{2\pi\theta}} e^{-\frac{(x_i-\theta)^2}{2\theta}}] \\
		    = & \sum_{i}^{n} [ \frac{-1}{2} \log 2\pi\theta -\frac{(x_i-\theta)^2}{2\theta}] \\
		    = & \frac{-n}{2} \log 2\pi\theta - \frac{1}{2\theta} \sum_{i}^{n}(x_i-\theta)^2 \\
		    = & -\frac{n}{2} \log 2\pi -\frac{-n}{2} \log \theta - \frac{1}{2\theta} [\sum_{i}^{n}x_i^2-\sum_{i}^{n}2x_i\theta + \sum_{i}^{n}\theta^2] \\
		    = & -\frac{n}{2} \log 2\pi -\frac{n}{2} \log \theta - \frac{\sum_{i}^{n}x_i^2}{2\theta} + \sum_{i}^{n}2x_i - \frac{n\theta}{2}
\end{align*} We know that, the likelihood for parameter $\theta$ is maximum when the first order derivative of the distribution function is $0$.
	\newline Thus, the $\theta_{MLE}$ can be estimated using
$\dfrac{d l(\theta)}{d\theta} = 0$
\begin{align*}
	\dfrac{d}{d\theta} [-\frac{n}{2} \log 2\pi -\frac{n}{2} \log \theta - \frac{\sum_{i}^{n}x_i^2}{2\theta} + \sum_{i}^{n}2x_i - \frac{n\theta}{2}] =& 0 \\
	\frac{-n}{2\theta} + \frac{\sum_{i}^{n}x_i^2}{2\theta^2} - \frac{n}{2} = & 0 \\
	-n\theta + \sum_{i}^{n}x_i^2 - n\theta^2 = & 0 \\
	n\theta^2 + n\theta - \sum_{i}^{n}x_i^2 = & 0 \\
	\text{This is a quadratic equation of } \theta, \text{roots are} \\
	\implies \hat{\theta} =& \frac{-n \pm \sqrt{n^2 + 4 n \sum_{i}^{n}x_i^2 }}{2n}
\end{align*}
	
\end{description}
\subsection{b.}
Given that $\hat{f}(x) = \frac{1}{n} \sum_{i=n}^{n} \frac{1}{h} K(\frac{x-X_i}{h})$.
\newline 
\begin{align*}
\textbf{E}_{X_1,X_2,..X_n}[\hat{f}(x)] =& \text{E} [\frac{1}{n} \sum_{i=n}^{n} \frac{1}{h} K(\frac{x-X_i}{h}) ] \\
  =& \frac{1}{n} \sum_{i=1}^{n}  \textbf{E}[\frac{1}{h}K(\frac{x - X_i }{h})] \\
  =& \textbf{E}[\frac{1}{h}K(\frac{x - X}{h})] \\
  =& \int K(\frac{x - t}{h} ) f(t) dt \\
   & \text{Substitute $z = \frac{x-t}{h} \implies t  = x - hz$ } \\
  =& \int K(z) f(x-hz) dz \\
   & \text{Using Taylors theorem, } f(x-a) = f(x) - af'(x) + \frac{a^2}{2}f''(x) + R \text{, where R is a  remainder }\\
  =& \int K(z) \big[f(x) - hzf'(x) + \frac{h^2z^2}{2}f''(x) + R\big] dz \\
  =& f(x) \int K(z) dz - h f'(x) \int zK(z)dz + \frac{h^2}{2} f''(x)\int [z^2 K(z) + R] dz \\
  & \text{We known that, by definition of kernel function} \int K(z)dz = 1\\ 
  & \text{and by Integration by parts rule,} \int z K(z) dz = 0 \\  
\textbf{E}[\hat{f}(x)]  =& f(x) + \frac{h^2}{2} f''(x) \int K(z) z^2 dz + R \\
\text{The bias is given by} \\
	{E}[\hat{f}(x)] - f(x) =& \frac{h^2}{2} f''(x)\int [z^2 K(z) + R] dz 
\end{align*}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{2. Naive Bayes}
\subsection{(a)}

Given: 
\\
  $X = {X_1, X_2, ... X_D}$ is a continuous random distribution.\\
  The label variable, Y, assumes two values \{0, 1\} and follows Bernoulli distribution. \\
  P (Y = 1) = $\pi \implies $ P(Y = 0) = 1 - $\pi$ \\
  The feature variable $X_j$ follows Gaussian Distribution of form $N(\mu_{jk}, \sigma_j)$.
 
 Using Bayes Rule:
 \begin{gather*}
 P(Y=1 | X) = \frac{P(X | Y=1)P(Y=1)}{P(X | Y=1)P(Y=1) + P(X_d | Y=0)P(Y=0)} \\
            = \cfrac{1}{1 + \cfrac{P(X | Y=0)P(Y=0)}{P(X | Y=1)P(Y=1)}} \\
  \text{Subsitituting \( P(Y=1) = \pi \) }\\
            = \cfrac{1}{1 + \cfrac{P(X | Y=0)(1-\pi)}{P(X | Y=1) \pi}} \\
  \text{By the naive assumption of conditional independence between the features}\\
            = \cfrac{1}{1 + \frac{1-\pi}{\pi} \times \cfrac{\prod_{i_1}^{D}P(X_j | Y=0)}{\prod_{j}^{D}P(X_j | Y=1)}} \\
            = \cfrac{1}{1 + \frac{1-\pi}{\pi} \times \prod_{j}^{D} \cfrac{P(X_j | Y=0)}{P(X_j | Y=1)}} \\
            = \cfrac{1}{1 + \frac{1-\pi}{\pi} \times \prod_{j}^{D} P(X_j | Y=0) \times \cfrac{1}{\prod_{j}^{D}P(X_j | Y=1)}}
 \end{gather*}
 After plugging these terms in the values from Guassian Distribution and reducing it to expected form
 \begin{gather*}
 P(Y=1 | X) = \frac{1}{1 + exp(ln(\cfrac{1 - \pi}{1}) + \sum_j{\cfrac{\mu_{1j}^2 - \mu_{0j}^2}{2 \sigma_j^2}} + \sum_j{\cfrac{\mu_{0j} - \mu_{1j}}{\sigma_j^2}x_i})} \\
 \end{gather*}
 This can be reduced to  $ \cfrac{1}{1 + exp(-\omega_0 + \omega^T X)} $, by following substitution
 \begin{gather*}
 \omega_0 = -ln(\cfrac{1 - \pi}{1}) - \sum_j{\cfrac{\mu_{1j}^2 - \mu_{0j}^2}{2 \sigma_j^2}} \\
 \mathbf{\omega} = \cfrac{\mu_{0j} - \mu_{1j}}{\sigma_j^2}
 \end{gather*}

\subsection{(b)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{3. Nearest Neighbor}

\subsection{(a)}

Data Preprocessing:
	The input data are normalized and standardized. The mean of the coordinates, denoted by $(\mu_x, \mu_y)$ and the standard deviations of co-ordinates, denoted by $(\sigma_x, \sigma_y)$, are computed for the given data points as follows 
	\begin{align*}
		\mu_x & = \frac{1}{n}\sum_{1}^{n} x_i   &  \sigma_x &= \sqrt{ \frac{1}{n-1} \sum_{1}^{n} (x_i - \mu_x)^2 } \\
		\mu_y & = \frac{1}{n}\sum_{1}^{n} y_i   &  \sigma_y &= \sqrt{\frac{1}{n-1} \sum_{1}^{n} (y_i - \mu_y)^2 }\\
	\end{align*}

For the given data points, values of these variables are:
	\begin{align*}
		n &= 13  \\
		(\mu_x, \mu_y )& = (12.7692, 12.3077)  \\
		(\sigma_x, \sigma_y) &= (20.7169, 25.9306)
	\end{align*}

The position of each students, $(x_i, y_i)$ is transformed to a new normalized co-ordinate system by shifting the origin to $(\mu_x, \mu_y)$ and scaling the magnitudes by $(\frac{1}{\sigma_x},   \frac{1}{\sigma_y})$ as follows:
\begin{equation*}
	(x_{i'}, y_{i'}) =  (\frac{x_i - \mu_x}{\sigma_x}, \frac{y_i - \mu_y}{\sigma_y})
\end{equation*}


For any two points, $(x_1, y_1)$ and $(x_2, y_2)$,


The $L_1$ distance between these two points is given by:
$L_1 = |x_1 - x_2| + |y_1 - y_2|)$ 

The $L_2$ distance between these two points is:
$L_2 = \sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}$

Given that the new students location is $ (20, 7)$. 
In the normalized-standardized coordinate system, it is transformed to position (0.3490, -0.2047).

Using the above formula, the distances are computed and the data are tabulated below:

\begin{tabular}{| c | c | c| c || r || r |}
	\hline			
	USCID  & $(x_i, y_i)$ & Major & $(x_{i'}, y_{i'})$ & $L_1$ $(0.3490, -0.2047)$ & $L_2$  $(0.3490, -0.2047)$\\
	\hline
	S1  &  (0, 49)  &  Mat & (-0.6164, 1.4150) & 0.965 + 1.620 = 2.585 & $\sqrt{0.932 + 2.623}$ = 1.886 \\
	S2  &  (-7, 32) &  Mat & (-0.9543, 0.7594) & 1.303 + 0.964 = 2.267 & $\sqrt{1.698 + 0.930}$ = 1.621 \\
	S3  &  (-9, 47) &  Mat & (-1.0508, 1.3379) & 1.400 + 1.543 = 2.942 & $\sqrt{1.959 + 2.380}$ = 2.083 \\
	S4  &  (29, 12) &  Ele & (0.7835, -0.0119)& 0.434 + 0.193 = 0.627 & $\sqrt{0.189 + 0.037}$ = 0.475  \\
	S5  &  (49, 31) &  Ele & (1.7488, 0.7209) & 1.400 + 0.926 = 2.325 & $\sqrt{1.960 + 0.857}$ = 1.678 \\
	S6  &  (37, 38) &  Ele & (1.1696, 0.9908) & 0.821 + 1.196 = 2.016 & $\sqrt{0.673 + 1.429}$ = 1.450 \\
	S7  &  (8, 9)   &  CSc & (-0.2302, -0.1276) & 0.579 + 0.077 = 0.656 & $\sqrt{0.335 + 0.006}$ = 0.584 \\
	S8  &  (13, -1) &  CSc & (0.0111, -0.5132) &  0.338 + 0.309 = 0.646 &$\sqrt{0.114 + 0.095}$ = 0.458\\
	S9  &  (-6, -3) &  CSc & (-0.9060, -0.5903) & 1.255 + 0.386 = 1.641 &$\sqrt{1.575 + 0.149}$ = 1.313 \\
	S10  & (-21, 12) & CSc & (-1.6300, -0.0119) & 1.979 + 0.193 = 2.172 & $\sqrt{3.917 + 0.037}$ = 1.988\\
	S11  & (27, -32) & Eco & (0.6869, -1.7087) & 0.338 + 1.504 = 1.842 & $\sqrt{0.114 + 2.262}$ = 1.541\\
	S12  &  (19,-14) &  Eco & (0.3008, -1.0145) & 0.048 + 0.810 = 0.858 & $\sqrt{0.002 + 0.656}$ = 0.811\\
	S13  &  (27,-20) &  Eco & (0.6869, -1.2459) & 0.338 + 1.041 = 1.379 & $\sqrt{0.114 + 1.084}$ = 1.095\\
	\hline
\end{tabular}


Our predictions of Major of student at $(20, 7)$ for $K= 1, 5$ are computed and tabulated below


\begin{tabular}{| c || c | c |}
	\hline
	Metric 			& K nearest students & Predicted major Class \\
	\hline
	K=1 using $L_1$ & S4 &  Electrical Engineering \\
	K=1 using $L_2$ & S8 & Computer Science \\
	K=5 using $L_1$ & S4, S7, S8, S12 and S13 & Computer Science \\
	K=5 using $L_2$ &  S8, S4, S7, S12 and S13 & Computer Science\\
	\hline
\end{tabular}

\subsection{(b)}
Given/Known facts from problem statement:
\\ There are D dimensions in the vector space
\\ $x$ is a data point, and there is a D-sphere centered at $x$
\\  $V$ is the volume of the sphere.
\\ $N$ is the total number of points in the vector space
\\ $N_c$ is the total number of points in the vector space which have label $c$
\\ $K$ is the total number of points inside the D-sphere
\\ $K_c$ is the total number of points inside the D-sphere which have label $c$
\\ $p(x | Y = c) = \frac{K_c}{N_c V}$
\\ $p(Y = c) = \frac{N_c}{N}$


\begin{description}
	\item[$\bullet$ Unconditional probability density $p(x)$] 
	Using the marginal probability distribution, the $p(x)$ can be rewritten as summation of all marginals.
	\begin{align*}
		p(x) & = \sum_{c=1}^{c=C} P(x, Y=c) \\
			 & = \sum_{c=1}^{c=C} P(x | Y=c) \times P(Y=c) \\
			 & = \sum_{c=1}^{c=C} \frac{K_c}{N_c V} \times \frac{N_c}{N} \\
			 & = \frac{1}{N V}\sum_{c=1}^{c=C} K_c \\
		p(x) & = \frac{1}{N} \times \frac{K}{V}
	\end{align*}

	\item[$\bullet$ Posterior probability of class membership]
	
	Using Bayes' rule, 
	\begin{align*}
		p(Y=c|x) & = \frac{p(x | Y=c) p(Y=c) }{p(x)}\\
	 		& = p(x | Y=c) \times p(Y=c) \times \frac{1}{p(x)} \\
			& = \frac{K_c}{N_c V} \times \frac{N_c}{N} \times \frac{NV}{K} \\
			& = \frac{K_c}{K}
	\end{align*}
\end{description}


%%%%%%%%%%%%%%%%%%%%%%%%%
\section{4. Decision Tree}
\subsection{(a)}

The entropy at the root node of decision tree is,\\
	\begin{align*}
	 E(root) = & E (H=2, L=2) \\	
	         = & -\frac{2}{4} \log_2\frac{2}{4} -\frac{2}{4} \log_2\frac{2}{4} \\
	         = & 1
	\end{align*}

Information Gain with attribute $Weather$ and $Traffic$ is
	\begin{align*}
		 Gain(Weather) = & E(root) - \frac{2}{4} E_{W=Sunny}(H=1, L=1)  \\ 
				 & - \frac{2}{4} E_{W=Rainy}(H=1, L=1) \\
				 = & 1 - \frac{1}{2} (-1) - \frac{1}{2} (-1) \\
		 Gain(Weather)	= & 0 \\
		 Gain(Traffic) = & E(root) - \frac{2}{4} E_{T=Heavy}(H=2, L=0) \\ & - \frac{2}{4} E_{T=Light}(H=0, L=2) \\
 		  = & 1 - 0 - 0 \\
		 Gain(Traffic) = & 1
	\end{align*}

Since Information Gain of $Traffic$ is higher than $Weather$, I will split based on $Traffic$ at first.

\subsection{(b)} $T_1$ - Tree built without normalizing the dataset. 
$T_2$ - Tree built after normalizing the dataset \\
Question: How are $T_1$ and $T_2$ related? \\
Answer : \\ 
Statement: $T_1$ and $T_2$ are similar.\\
Argument: The key component of the decision tree algorithm is the entropy of attributes. The normalization and standardization of dataset does not alter the entropy of the attributes, thus the Trees $T_2$ and  $T_1$ are similar.

\subsection{(c)}
Question : Prove that Gini Index is a better approximation than Cross Entropy. \\
Proof: \\
For a random variable with PMF $p$), \\
Gini Index is defined by $\sum_{k=1}^{K} p_k (1-p_k) $ \\
And the cross entropy is defined by $-\sum_{k=1}^{K} p_k \log p_k $ \\
Give that, these two functions are the approximations of misclassification error $\implies$ the function with smaller value is a better approximation.\\

Since the $p_k$ is defined in between $[0, 1]$, let us plot these functions in the domain $[0, 1]$:
\begin{tikzpicture}
	\draw[->] (-1,0) -- (4,0) node[right] {$x$};
	\draw[->] (0,-1) -- (0,4) node[above] {$y$};
	\draw[scale=4,domain=0:1,smooth,variable=\x,blue] plot ({\x},{\x * (1 - \x)});
	\draw[scale=4,domain=0.00001:1,smooth,variable=\y,red]  plot ({\y},{-\y * ln(\y)});
	\node at (1.8,1.8) {\footnotesize $y=-x\log(x)$};
	\node at (2.2,0.6) {\footnotesize $y=x(1-x)$};
\end{tikzpicture}

It is evident that the area under the curve $p_k (1-p_k)$ is smaller than the $p_k \log p_k $. \\
\begin{equation*}
	\int_{0}^{1} p_k (1-p_k) \space dp_k = \int_{0}^{1} [p_k - p_k^2] d_p \\
				= \frac{x^2}{2} - \frac{x^3}{3} \bigg|_0^1 \\
				= \frac{1}{6}
\end{equation*}
\begin{equation*}
	- \int_{0}^{1} p_k log p_k \space dp_k 
	= - \frac{p_k^2 \space log p_k}{2} + \frac{p_k^2}{4} \bigg|_0^1 
	= \frac{1}{4} 
\end{equation*}

Thus $\sum_{k=1}^{K} p_k (1-p_k) <= -\sum_{k=1}^{K} p_k \log p_k \\ \implies$  Gini Index is a better approximation than Cross Entropy


\section{4. Programming}
\subsection{(a) Data Inspection}
\begin{description}
	\item[1] How many attributes are there
	\newline There are 10 attributes in each record (including the id).
	
	\item[2] Do you think that all attributes are meaningful for the classification? If not, explain why.
	\newline The \textit{id} attribute is definitely not useful for classification. The reason is obvious since \textit{id} of a record is meant for identification and it does not explain the nature of glass (in other words, \textit{id} is not a function of its behavior).
	 Since I do not have expertise in the domain of glass manufacturing, I would not try to decide the usefulness or uselessness of other attributes in the dataset at this point of time, instead I would let my statistical learning model to learn it. Hence I will supply all the attributes to my learning machine.
	 
	 For the further improvements in the accuracy, we may ignore the attributes with zero variance. 
	 
	 \item[3] How many classes are? Class is a type of glass.
		 \newline There are \textbf{6} classes of glasses in the dataset provided on blackboard. Originally, in UCI repository there were \textbf{7} classes, but the class with label \textbf{4} is missing in the dataset provided in blackboard.
	
	\item[4] Please explain the class distribution. Which class is majority? Do you think that it can be considered as a uniform distribution?
	The distribution of classes are as follows:
	\textbf{Training Data}:\newline
	The training data has 196 examples. The distribution is: \newline
	\begin{tabular}{| c | c | c| }
		\hline
		Class & Count & Probability \\
		\hline \hline
		1     &  67   &  0.3418 \\ \hline
		2     &  72   &  0.3673 \\ \hline
		3     &  14   &  0.0714 \\ \hline
		5     &  10   &  0.0510 \\ \hline
		6     &   6   &  0.0306 \\ \hline
		7     &  26   &  0.1326 \\ \hline
	\end{tabular} \newline
	As we can infer from the probability column, the classes in training dataset are not distributed uniformly.

	\textbf{Testing Data}: \newline
	The testing data has 18 examples. The distribution is: \newline
	\begin{tabular}{| c | c | c| }
		\hline
		Class & Count & Probability \\
		\hline \hline
		1     &  3   &  $\frac{1}{6}$ \\ \hline
		2     &  3   &  $\frac{1}{6}$ \\ \hline
		3     &  3   &  $\frac{1}{6}$ \\ \hline
		5     &  3   &  $\frac{1}{6}$ \\ \hline
		6     &  3   &  $\frac{1}{6}$ \\ \hline
		7     &  3   &  $\frac{1}{6}$ \\ \hline
	\end{tabular} \newline
	As we can infer from the probability column, the classes in testing dataset are distributed uniformly with probability of $\frac{1}{6}$.
		
	
\end{description}
\subsection{(b) Performance Comparison}

\subsubsection{KNN}
	The accuracy is tabulated below \\
	\begin{tabular}{ | r | r| r| r|} \hline
		K  & Distance Measure & Training Accuracy & Testing Accuracy \\ \hline
		1  & $L_1$ & 0.7500 & 0.6667 \\ \hline
		3  & $L_1$ & 0.7397 & 0.1111 \\ \hline
		5  & $L_1$ & 0.6836 & 0.5555 \\ \hline
		7  & $L_1$ & 0.6887 & 0.5000 \\ \hline
		1  & $L_2$ & 0.7091 & 0.6111 \\ \hline
		3  & $L_2$ & 0.7193 & 0.6111 \\ \hline
		5  & $L_2$ & 0.6683 & 0.5555 \\ \hline
		7  & $L_2$ & 0.6734 & 0.5555 \\ \hline
	\end{tabular}
	\newline


\subsubsection{Naive Bayes}
The training accuracy of Naive Bayes classifier is $0.5510$ and the testing accuracy is $0.3333$.

\subsubsection{Discussion}
	The results from the classifier are not similar. From the above tabulated statistics, we can infer that the KNN classifier performed better than Naive Bayes. The best performing parameters of KNN classifier for the given dataset is when $K=1$ and $Distance Measure = L_1$. 
	

	My observation and statements for this exercise:
	
		The distribution of training examples are skewed. For instance Class with label "6" has only 6 examples but class with label "2" is dominating with 72 examples. This greatly altered the probability of the target classes. In practice, balancing the training data may be helpful when using Naive Bayes Classifier.
	
		The training data consists of only 196 labeled examples but 6 target classes. 196 examples may be very few training for distinguishing 6 different target classes. 
		
		There were 9 attributes in the dataset, few of them have zero or close to zero variance. These attributes do not provide any information for classification. Thus actual number of useful attributes are fewer than 9. 
		
		When the parameter K is increased, due to fewer examples of many different kinds (i.e. more types of classes and lesser training examples), the KNN classifier gets confused. This situation may get better when more examples can be added for training or the number of classes can be reduced.
		
		

\end{document}