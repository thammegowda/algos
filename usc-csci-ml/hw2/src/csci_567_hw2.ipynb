{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tg/anaconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13)\n",
      "['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
      " 'B' 'LSTAT']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "print(boston.data.shape)\n",
    "print(boston.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 1)\n"
     ]
    }
   ],
   "source": [
    "dfX = boston.data\n",
    "dfY = np.array([boston.target]).transpose()\n",
    "print(dfY.shape)\n",
    "assert dfX.shape[0] == dfY.shape[0]\n",
    "n = dfX.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tg/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:3: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "test_split_rule = [i % 7 == 0 for i in range(n)]\n",
    "train_split_rule = np.invert(test_split_rule)\n",
    "testX, testY = dfX[test_split_rule], dfY[test_split_rule]\n",
    "trainX, trainY = dfX[train_split_rule], dfY[train_split_rule]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Histograms and pearson correlation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_attrs = trainX.shape[1]\n",
    "attrs = [ trainX[:, i] for i in range(n_attrs)]\n",
    "distribs = [(a.mean(), a.std())for a in attrs]\n",
    "\n",
    "corrs = np.zeros(shape=(n_attrs, n_attrs))\n",
    "for i, a in enumerate(attrs):\n",
    "    for j in range(0, i + 1):\n",
    "        b = attrs[j]\n",
    "        # COV[A,B] = E[X - muX] E[Y - muY] = E[XY] - E[X]E[Y]\n",
    "        # Pearson CORR[A, B] = COV[A,B] / (sigmaX * sigmaY)\n",
    "        cov_ab = np.multiply(a, b).mean() - distribs[i][0] * distribs[j][0]\n",
    "        cor_ab = cov_ab / (distribs[i][1] * distribs[j][1])\n",
    "        corrs[i][j] = cor_ab\n",
    "\n",
    "print(\"Correlations\")\n",
    "print(pd.DataFrame(corrs))\n",
    "plt.imshow(corrs, cmap='hot', interpolation='nearest')\n",
    "plt.show()\n",
    "\n",
    "plt.hist(trainY, bins=bins)\n",
    "plt.title(\"Histogram of Price with %d bins\" % bins)\n",
    "plt.show()\n",
    "\n",
    "print(\"Histogram\")\n",
    "bins = 10\n",
    "for i, attr in enumerate(attrs):\n",
    "    plt.hist(attr, bins=bins)\n",
    "    plt.title(\"Histogram of '%s' with %d bins\" % (boston.feature_names[i], bins))\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learn Rate 0.0001\n",
      "#0, cost = 538.50243\n",
      "[ 1.10282596 -0.04495359  0.8220239   0.56757461  0.50598767  0.09673651\n",
      "  1.11622142  0.63148602  0.46484325  0.58576546  0.16509781  0.12465829\n",
      "  0.59073659  0.5383079 ]\n",
      "#100, cost = 21.886264\n",
      "#200, cost = 21.205122\n",
      "#300, cost = 21.055035\n",
      "#400, cost = 21.002407\n",
      "#500, cost = 20.978718\n",
      "#600, cost = 20.966388\n",
      "#700, cost = 20.959521\n",
      "#800, cost = 20.955589\n",
      "#900, cost = 20.953313\n",
      "#1000, cost = 20.95199\n",
      "[ 22.46351039  -0.9603431    1.03157636  -0.21743147   0.92802198\n",
      "  -1.61141017   2.73312084  -0.27506989  -3.11621861   2.3747743   -1.78702\n",
      "  -1.88317389   0.82828193  -3.6714615 ]\n",
      "#1100, cost = 20.95122\n",
      "#1200, cost = 20.950771\n",
      "#1300, cost = 20.95051\n",
      "#1400, cost = 20.950357\n",
      "#1500, cost = 20.950269\n",
      "#1600, cost = 20.950217\n",
      "#1700, cost = 20.950187\n",
      "#1800, cost = 20.950169\n",
      "#1900, cost = 20.950159\n",
      "#2000, cost = 20.950153\n",
      "[ 22.46351039  -0.96686743   1.04451163  -0.17646787   0.92183647\n",
      "  -1.62007604   2.72620882  -0.26775038  -3.11274196   2.47632944\n",
      "  -1.9024432   -1.88769607   0.82932247  -3.67507135]\n",
      "#2100, cost = 20.950149\n",
      "#2200, cost = 20.950147\n",
      "#2300, cost = 20.950146\n",
      "#2400, cost = 20.950145\n",
      "Converged with tolerance 0.000001 \n",
      "\n",
      "\n",
      "\n",
      "Train MSE:: 20.9501454704\n",
      "Test MSE:: 33.3708832832\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def predict(X, W):\n",
    "    return np.matmul(X, W)\n",
    "\n",
    "def MSECost(Y2, Y1):\n",
    "    return float(np.sum((Y2 - Y1) ** 2) / len(Y2))\n",
    "\n",
    "def gradient_desc(X, Y, W, alpha,\n",
    "                  num_iter = 1000, conv_tol=0.01, print_interval = 100):\n",
    "    c = float('inf')\n",
    "    print(\"Learn Rate\", alpha)\n",
    "    for i in range(num_iter):\n",
    "        predY = predict(X, W)\n",
    "        diff = predY - Y\n",
    "        delta = np.sum(np.multiply(X, diff), axis=0) # sum top to bottom for each attribute\n",
    "        delta = np.array([delta]).transpose()        # restore vector shape of (n_attr x 1)\n",
    "        \n",
    "        W = (W - alpha * delta)\n",
    "        if i % print_interval == 0:\n",
    "            predY = predict(X, W)\n",
    "            #print(np.concatenate((Y, predY), axis=1))\n",
    "            newcost = MSECost(predY, Y)\n",
    "            print(\"#%d, cost = %.8g\" % (i, newcost))\n",
    "            if np.isnan(newcost) or np.isinf(newcost) or np.isneginf(newcost):\n",
    "                raise Exception(\"ERROR: number overflow, please adjust learning rate\")\n",
    "            diff = abs(newcost - c)\n",
    "            c = newcost\n",
    "            if diff < conv_tol:\n",
    "                print(\"Converged with tolerance %f \" % conv_tol)\n",
    "                break            \n",
    "        if i % (print_interval * 10) == 0:\n",
    "            print(W.flatten())\n",
    "    return W\n",
    "\n",
    "\n",
    "# compute means and stds\n",
    "class LinearRegression(object):\n",
    "    \n",
    "    def __init__(self, X, Y, learn_rate=0.001, num_iter=1000, conv_tol=0.1):\n",
    "        \n",
    "        self.means = X.mean(axis=0)\n",
    "        self.stds = X.std(axis=0)        \n",
    "        X = self.normalize(X)\n",
    "        self.n_attrs = X.shape[1]\n",
    "        W = np.random.rand(self.n_attrs, 1)\n",
    "        self.W = gradient_desc(X, Y, W, alpha=learn_rate,\n",
    "                                num_iter=num_iter, conv_tol=conv_tol)\n",
    "    \n",
    "    def normalize(self, X):\n",
    "        X = (X - self.means) / self.stds\n",
    "        # Bias is added as a weight to simplify the calculations\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "        return X\n",
    "    \n",
    "    def predict(self, X, normalize=True):\n",
    "        if normalize:\n",
    "            X = self.normalize(X)\n",
    "        return np.matmul(X, self.W)\n",
    "            \n",
    "    \n",
    "alpha = 0.0001\n",
    "conv_tol = 0.000001\n",
    "num_iter = 10000\n",
    "linreg = LinearRegression(trainX, trainY, alpha, num_iter, conv_tol)\n",
    "\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "predY = linreg.predict(trainX)\n",
    "train_mse_cost = MSECost(predY, trainY)\n",
    "print('Train MSE::', train_mse_cost)\n",
    "\n",
    "\n",
    "predY = linreg.predict(testX)\n",
    "test_mse_cost = MSECost(predY, testY)\n",
    "print('Test MSE::', test_mse_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.multiply(trainX[:, 1], trainX[:, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
