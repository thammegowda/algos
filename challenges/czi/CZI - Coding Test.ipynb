{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: [Thamme Gowda](http://www-scf.usc.edu/~tnarayan)\n",
    "\n",
    "# Data Exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input(dir_path, ext=\".txt\"):\n",
    "    for f_path in filter(lambda x: x.endswith(ext), os.listdir(dir_path)):\n",
    "        with open(os.path.join(dir_path, f_path)) as ptr:\n",
    "            yield int(f_path.replace(ext, '')), ptr.read().strip()\n",
    "\n",
    "data = [x for x in read_input(\"data\")]\n",
    "data = sorted(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(txt, clean=True, lower=False):\n",
    "    if clean:\n",
    "        # replace all non alphabets with spaces\n",
    "        txt = re.sub('[^a-zA-Z]+', ' ', txt)\n",
    "    if lower:\n",
    "        txt = txt.lower()\n",
    "    return txt.split()\n",
    "\n",
    "unigrams = {}\n",
    "bigrams = {}\n",
    "uni_counts = Counter()\n",
    "bi_counts = Counter()\n",
    "for id, txt in data:\n",
    "    tokens = tokenize(txt, lower=True)\n",
    "    bi_tokens = list(zip(tokens, tokens[1:]))\n",
    "    unigrams[id] = tokens\n",
    "    bigrams[id] =  bi_tokens\n",
    "    uni_counts.update(tokens)\n",
    "    bi_counts.update(bi_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 37999),\n",
       " ('of', 17400),\n",
       " ('and', 12047),\n",
       " ('in', 10617),\n",
       " ('a', 10510),\n",
       " ('to', 10308),\n",
       " ('is', 7548),\n",
       " ('for', 4959),\n",
       " ('with', 4122),\n",
       " ('as', 3588),\n",
       " ('that', 3517),\n",
       " ('by', 3339),\n",
       " ('are', 3227),\n",
       " ('at', 2988),\n",
       " ('be', 2954),\n",
       " ('this', 2713),\n",
       " ('on', 2689),\n",
       " ('from', 2394),\n",
       " ('can', 1936),\n",
       " ('was', 1734),\n",
       " ('an', 1719),\n",
       " ('which', 1630),\n",
       " ('it', 1454),\n",
       " ('we', 1411),\n",
       " ('energy', 1374),\n",
       " ('fig', 1291),\n",
       " ('have', 1237),\n",
       " ('or', 1213),\n",
       " ('s', 1172),\n",
       " ('has', 1148),\n",
       " ('high', 1136),\n",
       " ('using', 1100),\n",
       " ('these', 1090),\n",
       " ('were', 1076),\n",
       " ('used', 1073),\n",
       " ('cells', 1061),\n",
       " ('c', 1049),\n",
       " ('m', 1028),\n",
       " ('been', 1026),\n",
       " ('e', 973),\n",
       " ('cell', 969),\n",
       " ('also', 966),\n",
       " ('d', 953),\n",
       " ('temperature', 926),\n",
       " ('two', 925),\n",
       " ('between', 923),\n",
       " ('not', 899),\n",
       " ('than', 884),\n",
       " ('i', 861),\n",
       " ('such', 836)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_counts.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('of', 'the'), 5526),\n",
       " (('in', 'the'), 2911),\n",
       " (('to', 'the'), 2051),\n",
       " (('on', 'the'), 1243),\n",
       " (('and', 'the'), 1232),\n",
       " (('can', 'be'), 1164),\n",
       " (('for', 'the'), 1023),\n",
       " (('from', 'the'), 988),\n",
       " (('that', 'the'), 875),\n",
       " (('with', 'the'), 863),\n",
       " (('at', 'the'), 812),\n",
       " (('is', 'the'), 730),\n",
       " (('in', 'fig'), 729),\n",
       " (('by', 'the'), 724),\n",
       " (('of', 'a'), 718),\n",
       " (('in', 'a'), 713),\n",
       " (('due', 'to'), 707),\n",
       " (('with', 'a'), 632),\n",
       " (('to', 'be'), 608),\n",
       " (('it', 'is'), 606),\n",
       " (('as', 'a'), 560),\n",
       " (('has', 'been'), 510),\n",
       " (('shown', 'in'), 510),\n",
       " (('as', 'the'), 498),\n",
       " (('in', 'this'), 471),\n",
       " (('such', 'as'), 463),\n",
       " (('et', 'al'), 458),\n",
       " (('to', 'a'), 416),\n",
       " (('between', 'the'), 410),\n",
       " (('is', 'a'), 385),\n",
       " (('have', 'been'), 376),\n",
       " (('the', 'same'), 339),\n",
       " (('used', 'to'), 338),\n",
       " (('the', 'sample'), 334),\n",
       " (('and', 'a'), 322),\n",
       " (('for', 'a'), 318),\n",
       " (('which', 'is'), 317),\n",
       " (('based', 'on'), 312),\n",
       " (('by', 'a'), 292),\n",
       " (('i', 'e'), 288),\n",
       " (('number', 'of'), 275),\n",
       " (('the', 'first'), 266),\n",
       " (('a', 'single'), 253),\n",
       " (('into', 'the'), 252),\n",
       " (('fig', 'a'), 237),\n",
       " (('using', 'a'), 236),\n",
       " (('than', 'the'), 232),\n",
       " (('where', 'the'), 231),\n",
       " (('this', 'is'), 229),\n",
       " (('through', 'the'), 228)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_counts.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of docs =  100\n"
     ]
    }
   ],
   "source": [
    "ids = [x[0] for x in data]\n",
    "N = len(ids)\n",
    "# check that ids are sorted\n",
    "assert not list(filter(lambda p: p[0] > p[1],  zip(ids, ids[1:])))\n",
    "print(\"Number of docs = \", N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 : Find common words between documents\n",
    "\n",
    "For this task, you need to generate a matrix, where each entry contains the number of unique\n",
    "common tokens (words) between each pair of documents. The output should include no headers for\n",
    "rows or columns. The matrix should be symmetric, and follow the numbering conventions of the files.\n",
    "\n",
    "The diagonal entries would be the count of unique terms in each document.\n",
    "For example, the first number on the first line is the number of unique terms in 001.txt, the second\n",
    "number on the second line is the number of unique terms in 002.txt, etc.\n",
    "Similarly, the first element on the second line would be the number of unique terms that appear in\n",
    "both 001.txt and 002.txt, the 23rd number on the 16th line is the number of unique common terms\n",
    "between 023.txt and 016.txt, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix is symmetric\n"
     ]
    }
   ],
   "source": [
    "table = np.zeros((N, N), dtype=int)\n",
    "uni_sets = {}\n",
    "\n",
    "for id in unigrams:\n",
    "    uni_sets[id] = set(unigrams[id])\n",
    "\n",
    "for i in ids:\n",
    "    for j in ids:\n",
    "        if i == j:\n",
    "            # size of the set\n",
    "            table[i-1][i-1] = len(uni_sets[i])\n",
    "        else:\n",
    "            # intersection of two sets and size of the resulting set\n",
    "            table[i-1][j-1] = len(uni_sets[i] & uni_sets[j])\n",
    "            \n",
    "## assert that matrix is symmteric\n",
    "for i in range(N):\n",
    "    for j in range(N):\n",
    "        assert table[i][j] == table[j][i]\n",
    "print(\"Matrix is symmetric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1185  404  287 ...,  252  384  252]\n",
      " [ 404 1361  279 ...,  252  431  256]\n",
      " [ 287  279  821 ...,  351  302  246]\n",
      " ..., \n",
      " [ 252  252  351 ...,  694  278  218]\n",
      " [ 384  431  302 ...,  278 1234  276]\n",
      " [ 252  256  246 ...,  218  276  600]]\n"
     ]
    }
   ],
   "source": [
    "OUT_FILE = 'task1-out.txt'\n",
    "np.savetxt(OUT_FILE, table, fmt=\"%d\", delimiter=' ') \n",
    "#!cat {OUT_FILE} | pr -w 80\n",
    "#!fold -w 80 -s {OUT_FILE}\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Identifying words by frequency\n",
    "\n",
    "A bigram is sequence of two consecutive tokens. The previous sentence, for example, contains the bigrams: (a bigram), (bigram is), (is sequence), (sequence of), (of two), (two consecutive), and (consecutive tokens).\n",
    "Across the entire corpus find (1) the top 50 most frequent unigrams (single tokens), and (2) the top 50 most frequent bigrams.\n",
    "\n",
    "The output should be a list of 100 lines, where the first 50 lines contain a single term each, in order of frequency, followed by 50 lines containing two terms each, in order of the bigram frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1\tthe\r\n",
      "     2\tof\r\n",
      "     3\tand\r\n",
      "     4\tin\r\n",
      "     5\ta\r\n",
      "     6\tto\r\n",
      "     7\tis\r\n",
      "     8\tfor\r\n",
      "     9\twith\r\n",
      "    10\tas\r\n",
      "    11\tthat\r\n",
      "    12\tby\r\n",
      "    13\tare\r\n",
      "    14\tat\r\n",
      "    15\tbe\r\n",
      "    16\tthis\r\n",
      "    17\ton\r\n",
      "    18\tfrom\r\n",
      "    19\tcan\r\n",
      "    20\twas\r\n",
      "    21\tan\r\n",
      "    22\twhich\r\n",
      "    23\tit\r\n",
      "    24\twe\r\n",
      "    25\tenergy\r\n",
      "    26\tfig\r\n",
      "    27\thave\r\n",
      "    28\tor\r\n",
      "    29\ts\r\n",
      "    30\thas\r\n",
      "    31\thigh\r\n",
      "    32\tusing\r\n",
      "    33\tthese\r\n",
      "    34\twere\r\n",
      "    35\tused\r\n",
      "    36\tcells\r\n",
      "    37\tc\r\n",
      "    38\tm\r\n",
      "    39\tbeen\r\n",
      "    40\te\r\n",
      "    41\tcell\r\n",
      "    42\talso\r\n",
      "    43\td\r\n",
      "    44\ttemperature\r\n",
      "    45\ttwo\r\n",
      "    46\tbetween\r\n",
      "    47\tnot\r\n",
      "    48\tthan\r\n",
      "    49\ti\r\n",
      "    50\tsuch\r\n",
      "    51\tof the\r\n",
      "    52\tin the\r\n",
      "    53\tto the\r\n",
      "    54\ton the\r\n",
      "    55\tand the\r\n",
      "    56\tcan be\r\n",
      "    57\tfor the\r\n",
      "    58\tfrom the\r\n",
      "    59\tthat the\r\n",
      "    60\twith the\r\n",
      "    61\tat the\r\n",
      "    62\tis the\r\n",
      "    63\tin fig\r\n",
      "    64\tby the\r\n",
      "    65\tof a\r\n",
      "    66\tin a\r\n",
      "    67\tdue to\r\n",
      "    68\twith a\r\n",
      "    69\tto be\r\n",
      "    70\tit is\r\n",
      "    71\tas a\r\n",
      "    72\thas been\r\n",
      "    73\tshown in\r\n",
      "    74\tas the\r\n",
      "    75\tin this\r\n",
      "    76\tsuch as\r\n",
      "    77\tet al\r\n",
      "    78\tto a\r\n",
      "    79\tbetween the\r\n",
      "    80\tis a\r\n",
      "    81\thave been\r\n",
      "    82\tthe same\r\n",
      "    83\tused to\r\n",
      "    84\tthe sample\r\n",
      "    85\tand a\r\n",
      "    86\tfor a\r\n",
      "    87\twhich is\r\n",
      "    88\tbased on\r\n",
      "    89\tby a\r\n",
      "    90\ti e\r\n",
      "    91\tnumber of\r\n",
      "    92\tthe first\r\n",
      "    93\ta single\r\n",
      "    94\tinto the\r\n",
      "    95\tfig a\r\n",
      "    96\tusing a\r\n",
      "    97\tthan the\r\n",
      "    98\twhere the\r\n",
      "    99\tthis is\r\n",
      "   100\tthrough the"
     ]
    }
   ],
   "source": [
    "MAX = 50\n",
    "OUT_FILE = 'task2-out.txt'\n",
    "lines = []\n",
    "for term, count in uni_counts.most_common(MAX):\n",
    "    lines.append(term)\n",
    "\n",
    "assert len(lines) == 50\n",
    "for term, count in bi_counts.most_common(MAX):\n",
    "    lines.append(' '.join(term))\n",
    "\n",
    "assert len(lines) == 100\n",
    "with open(OUT_FILE, 'w') as out:\n",
    "    out.write('\\n'.join(lines))\n",
    "!cat -n {OUT_FILE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
